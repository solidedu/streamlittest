#!/bin/bash

# Define your bucket name, prefix, and download directory
BUCKET_NAME="datav"
PREFIX="fasdfsdfasdf/sdfhg"
DOWNLOAD_DIR="/var/snap/amazon-ssm-agent/23/files"

# Ensure the download directory exists
mkdir -p "$DOWNLOAD_DIR"

# Clean up download directory by removing all files
rm -rf "$DOWNLOAD_DIR"/*

# List and download the latest object from S3 and capture its timestamp
latest_file_info=$(/usr/local/bin/aws s3api list-objects-v2 \
    --bucket $BUCKET_NAME --prefix $PREFIX \
    --query 'Contents | sort_by(@, &LastModified)[-1]' \
    --output json)

# Extract the key and last modified timestamp
key=$(echo "$latest_file_info" | jq -r '.Key')
timestamp=$(echo "$latest_file_info" | jq -r '.LastModified')

# Check if the key was retrieved
if [ -z "$key" ] || [ "$key" == "None" ]; then
    echo "Error: No valid key found."
    exit 1
fi

# Download the latest file to the download directory
/usr/local/bin/aws s3 cp "s3://$BUCKET_NAME/$key" "$DOWNLOAD_DIR"
if [ $? -ne 0 ]; then
    echo "Error: Failed to download the latest file from S3."
    exit 1
fi

# Log the downloaded file and its timestamp for the inference script
echo "$DOWNLOAD_DIR/$(basename $key) $timestamp" > latest_downloaded_file_with_timestamp.txt

echo "Latest file downloaded successfully: $key"








import pandas as pd
from datetime import datetime
import pytz
import os
import torch
from torchvision.io import read_image

# Function to convert S3 UTC time to EST
def convert_to_est(s3_time):
    utc_time = datetime.strptime(s3_time, '%Y-%m-%dT%H:%M:%S.%fZ')
    est = pytz.timezone('US/Eastern')
    return utc_time.astimezone(est).strftime('%Y-%m-%d %H:%M:%S')

# Load the file path and S3 timestamp for the latest downloaded file
with open('/var/snap/amazon-ssm-agent/23/latest_downloaded_file_with_timestamp.txt', 'r') as f:
    # Read the line and split into file path and S3 timestamp
    line = f.readline().strip()
    file_path, s3_time = line.split(maxsplit=1)

# Convert the S3 timestamp to EST
est_timestamp = convert_to_est(s3_time)

# Initialize lists for predictions and corresponding metadata
predictions = []
filepaths = []
timestamps = []  # List to store individual S3 timestamps

model.eval()
with torch.no_grad():
    # Perform inference on the single image
    images = read_image(file_path).float().to(device)
    
    # Perform inference
    outputs = model(images).clone().detach().cpu()
    outputs = inference_dataset.reverse_scale(outputs)
    
    # Store predictions, filepaths, and the S3 timestamp for each prediction
    predictions.append(outputs.item())  # Append the single prediction
    filepaths.append(file_path)
    timestamps.append(est_timestamp)  # Assign the unique timestamp for the file

# Save the predictions, filepaths, and timestamps as a DataFrame
df = pd.DataFrame({
    'Filepath': filepaths, 
    'Prediction': predictions,
    'Timestamp': timestamps  # Add the unique timestamp for each file
})

# Save the DataFrame as a CSV file
df.to_csv('inference_predictions.csv', index=False)


